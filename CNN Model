import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import keras.backend as K #to define custom loss function

import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')

from pprint import pprint
from collections import defaultdict
import openslide
from openslide import OpenSlide

from glob import glob

from sklearn.model_selection import train_test_split
from tqdm import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.layers import GlobalMaxPooling2D
from keras.models import load_model

print(keras.__version__)

train_df = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')
test_df  = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')
train_df.head()

# Specify patient_ids to remove
patient_ids_to_remove = ['006388', '008e5c', '00c058', '01adc5']

# Filter out the rows with specified patient_ids
train_df = train_df[~train_df['patient_id'].isin(patient_ids_to_remove)].reset_index(drop=True)

# Print the cleaned DataFrame
print("Cleaned DataFrame:")
print(train_df.head())

##### CLASS DISTRIBUTION

plt.style.use('Solarize_Light2')
labels = train_df.groupby('label')['label'].count().div(len(train_df)).mul(100)
counts = train_df['label'].value_counts()  # Get counts for each class

# Create a single plot for the distribution of the target variable
plt.figure(figsize=(8, 5))
bar_plot = sns.barplot(x=labels.index, y=labels.values)
plt.title("Distribution of a Target Variable")
plt.ylabel("%")
plt.xlabel("Label")

# Set y-axis limits to increase the length of the bars
plt.ylim(0, 80)  

# Adding labels to each bar with adjusted spacing
for index, p in enumerate(bar_plot.patches):
    label = labels.index[index]
    count = counts[label]
    percentage = int(labels[label])
    bar_plot.annotate(f'{count} ({percentage}%)',
                      (p.get_x() + p.get_width() / 2., p.get_height() + 2),  # Adjusted y position
                      ha='center', va='bottom', fontsize=10)

plt.show()

print('Train Size = {}'.format(len(train_df)))
print('Test Size = {}'.format(len(test_df)))

train_images = glob("/kaggle/input/mayo-clinic-strip-ai/train/*")
test_images = glob("/kaggle/input/mayo-clinic-strip-ai/test/*")
other_images = glob("/kaggle/input/mayo-clinic-strip-ai/other/*")
print(f"Number of images in a training set: {len(train_images)}")
print(f"Number of images in a training set: {len(test_images)}")
print(f"Number of other: {len(other_images)}")

# Filtering out images based on the cleaned patient_ids in train_df
images_to_remove = [
    '/kaggle/input/mayo-clinic-strip-ai/train/006388_0.tif',
    '/kaggle/input/mayo-clinic-strip-ai/train/008e5c_0.tif',
    '/kaggle/input/mayo-clinic-strip-ai/train/00c058_0.tif',
    '/kaggle/input/mayo-clinic-strip-ai/train/01adc5_0.tif',
]

# Remove images associated with the patient_ids
train_images = [img for img in train_images if img not in images_to_remove]

# Check the total number of images after deletion
total_images_after_deletion = len(train_images)
print("Total number of images after deletion:", total_images_after_deletion)

# Print the paths of the cleaned list of images
print("First 5 image paths after deletion:")
print(train_images[:5])

img_prop = defaultdict(list)

for i, path in enumerate(train_images):
    img_path = train_images[i]
    slide = OpenSlide(img_path)    
    img_prop['image_id'].append(img_path[-12:-4])
    img_prop['width'].append(slide.dimensions[0])
    img_prop['height'].append(slide.dimensions[1])
    img_prop['size'].append(round(os.path.getsize(img_path) / 1e6, 2))
    img_prop['path'].append(img_path)

image_data = pd.DataFrame(img_prop)
image_data['img_aspect_ratio'] = image_data['width']/image_data['height']
image_data.sort_values(by='image_id', inplace=True)
image_data.reset_index(inplace=True, drop=True)

image_data = image_data.merge(train_df, on='image_id')
image_data.head()

plt.style.use('Solarize_Light2')

fig, ax = plt.subplots(1,2, figsize=(16,5))
sns.histplot(x='size', data = image_data, bins=100, ax=ax[0])
ax[0].set_title("Distribution of size"), ax[0].set_ylabel("%")
sns.histplot(x='img_aspect_ratio', data = image_data, bins=100, ax=ax[1])
ax[1].set_title("Image aspect ratio"), ax[1].set_ylabel("%")
plt.show()

from PIL import Image
Image.MAX_IMAGE_PIXELS = None 

CE_imgs = image_data.loc[image_data['label']=='CE','path']
LAA_imgs = image_data.loc[image_data['label']=='LAA','path']


plt.style.use('default')
fig, axes = plt.subplots(1,5, figsize=(16,16))
train_images
for ax in axes.reshape(-1):
    img_path = np.random.choice(CE_imgs)
    img = Image.open(img_path)   
    img.thumbnail((300,300), Image.Resampling.LANCZOS)
    ax.imshow(img), ax.set_title("target: CE")
plt.show()

fig, axes = plt.subplots(1,5, figsize=(16,16))
train_images
for ax in axes.reshape(-1):
    img_path = np.random.choice(LAA_imgs)
    img = Image.open(img_path)   
    img.thumbnail((300,300), Image.Resampling.LANCZOS)
    ax.imshow(img), ax.set_title("target: LAA")
plt.show()

slide = OpenSlide('/kaggle/input/mayo-clinic-strip-ai/train/026c97_0.tif') # opening a full slide

region = (2500, 2000) # location of the top left pixel
level = 0 # level of the picture (we have only 0)
size = (3500, 3500) # region size in pixels

region = slide.read_region(region, level, size)
image = region.resize((512, 512))
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.show()

train_df["file_path"] = train_df["image_id"].apply(lambda x: "../input/mayo-clinic-strip-ai/train/" + x + ".tif")
test_df["file_path"]  = test_df["image_id"].apply(lambda x: "../input/mayo-clinic-strip-ai/test/" + x + ".tif")

# labelling CE class as 1 and LAA as 0
train_df["target"] = train_df["label"].apply(lambda x : 1 if x=="CE" else 0)
train_df.head()

%%time
def preprocess(image_path):
    slide=OpenSlide(image_path)
    region= (2500,2500)    
    size  = (5000, 5000)
    image = slide.read_region(region, 0, size)
    image = image.resize((128, 128))
    image = np.array(image)    
    return image

X_train=[]
for i in tqdm(train_df['file_path']):
    x1=preprocess(i)
    X_train.append(x1)

Y_train=[]    
Y_train=train_df['target']

X_train=np.array(X_train)
X_train=X_train/255.0
Y_train = np.array(Y_train)

## Splitting data
x_train,x_test,y_train,y_test=train_test_split(X_train,Y_train, test_size=0.3, random_state=42)

print(x_train.shape)

plt.imshow(x_train[0])

def f1_score(y_true, y_pred): 
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

#Revised Version
from keras import metrics
from keras.layers import Dense

model = Sequential()
input_shape = (128, 128, 4)

model.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'valid', activation = 'relu', input_shape = input_shape))
model.add(MaxPooling2D())
model.add(Conv2D(filters=64, kernel_size = (3,3), strides =1, padding = 'valid', activation = 'relu'))
model.add(Conv2D(filters=64, kernel_size = (3,3), strides =1, padding = 'valid', activation = 'relu'))
model.add(Conv2D(filters=64, kernel_size = (3,3), strides =1, padding = 'valid', activation = 'relu'))
model.add(Conv2D(filters=128, kernel_size = (3,3), strides =1, padding = 'valid', activation = 'relu'))
model.add(Conv2D(filters=128, kernel_size = (3,3), strides =1, padding = 'valid', activation = 'relu'))

model.add(Dropout(0.13))
model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.13))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(50, activation = 'relu'))
model.add(Dense(1 , activation="sigmoid"))

model.compile(
   loss = tf.keras.losses.BinaryCrossentropy(),
    metrics=[metrics.binary_accuracy,f1_score],
    optimizer = tf.keras.optimizers.Adam(1e-3))

model.summary()

#from keras import metrics

#model = Sequential()
#input_shape = (128, 128, 4)

#model.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'valid', activation = 'relu', input_shape = input_shape))
#model.add(MaxPooling2D())
#model.add(Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'valid', activation = 'relu'))
#model.add(Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'valid', activation = 'relu'))
#model.add(Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'valid', activation = 'relu'))
#model.add(Conv2D(filters=128, kernel_size = (3,3), strides =2, padding = 'valid', activation = 'relu'))
#model.add(Conv2D(filters=128, kernel_size = (3,3), strides =2, padding = 'valid', activation = 'relu'))

#model.add(Dropout(0.13))
#model.add(Flatten())
#model.add(Dense(256, activation = 'relu'))
#model.add(Dropout(0.13))
#model.add(Dense(100, activation = 'relu'))
#model.add(Dense(50, activation = 'relu'))
#model.add(Dense(1 , activation="sigmoid"))

#model.compile(
    #loss = tf.keras.losses.BinaryCrossentropy(),
    #metrics=[metrics.binary_accuracy,f1_score],
    #optimizer = tf.keras.optimizers.Adam(1e-3))

dot_img_file = 'model.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)

from sklearn.utils import compute_class_weight
train_classes = Y_train
class_weights = compute_class_weight(
                                        class_weight = "balanced",
                                        classes = np.unique(train_classes),
                                        y = train_classes                                                    
                                    )
class_weights = dict(zip(np.unique(train_classes), class_weights))
class_weights

callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='our_cnn_best.h5',
    monitor='val_binary_accuracy',
    mode='max',
    save_best_only=True, verbose=1)

model.fit(
    x_train,
    y_train,
    epochs = 10,
    batch_size=20,
    validation_data = (x_test,y_test),
    class_weight= class_weights,
    callbacks = callback
)

best_cnn = load_model('/kaggle/working/our_cnn_best.h5', custom_objects={"f1_score": f1_score })
best_cnn.evaluate(x_test,y_test)

# Plot confusion matrices for benchmark and transfer learning models
from sklearn.metrics import confusion_matrix
import seaborn as sns

plt.figure(figsize=(15, 5))

preds = best_cnn.predict(x_test)
preds = (preds >= 0.5).astype(np.int32)

cm = confusion_matrix(y_test, preds)
df_cm = pd.DataFrame(cm, index=['LAA', 'CE'], columns=['LAA', 'CE'])
plt.subplot(121)
plt.title("Confusion matrix for our model\n")
sns.heatmap(df_cm, annot=True, fmt="d", cmap="YlGnBu")
plt.ylabel("Predicted")
plt.xlabel("Actual")

test1=[]
for i in test_df['file_path']:
    x1=preprocess(i)
    test1.append(x1)
    print(i)
    
test1=np.array(test1)

cnn_pred=model.predict(test1)
cnn_pred

sub = pd.DataFrame(test_df["patient_id"].copy())
sub["CE"] = cnn_pred
sub["LAA"] = 1- sub["CE"]

sub = sub.groupby("patient_id").mean()
sub = sub[["CE", "LAA"]].round(6).reset_index()
sub

label_counts = train_df["label"].value_counts()
print(label_counts)

sub["prediction"] = sub["CE"].apply(lambda x : "CE" if x>=0.5 else "LAA")
sub["actual"] = ["CE", "CE", "LAA","LAA"]
print(sub)

def weighted_log_loss(y_true, y_pred, weights, N):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    weights = np.array(weights)
    N = np.array(N) 

    # Compute log loss for each class
    log_loss_per_class = -np.sum(y_true / N[:, np.newaxis] * np.log(y_pred), axis=0)

    # Weighted log loss
    weighted_log_loss_value = np.sum(weights * log_loss_per_class) / np.sum(weights)
 
    return weighted_log_loss_value

y_true = np.array([[1, 0],
                [1, 0],
                [0, 1],
                [0, 1]])
    
N = [545,545,205,205]

y_pred = np.clip(sub.iloc[:, [1, 2]].values, 1e-15, 1 - 1e-15)

weights = np.array([1,1]) 

log_loss = weighted_log_loss(y_true, y_pred, weights, N)
print("log_loss:",log_loss)
